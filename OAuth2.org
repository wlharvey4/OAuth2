# -*- mode:org; -*-

#+title:All About OAuth2
#+subtitle:{{{version}}} {{{date}}}
#+author:lolh
#+date:2020-12-18 09:38
#+macro:version Version 0.0.1
#+macro:upload-date (eval (current-time-string))
#+bucket:pinecone-forest.com

{{{version}}} {{{date}}}

#+texinfo:@insertcopying


* Introduction
:PROPERTIES:
:unnumbered: t
:END:
* OAuth2 on GitHub
** GitHub API Authentication
   :PROPERTIES:
   :uri:      https://dev.to/gr2m/github-api-authentication-introduction-39dj
   :author:   Gregor Martynus
   :date:     2020-01-10
   :END:
*** GitHub API Authentication---Introduction
    :PROPERTIES:
    :uri:      https://dev.to/gr2m/github-api-authentication-introduction-39dj
    :END:
This is  the first post of  a series about different  Authentication strategies
for GitHub's APIs.  Strategies to be discussed:
 1. Personal Access Tokens
 2. GitHub Actions
 3. Username and Password (basic)
 4. OAuth
 5. WebHooks
 6. GitHub Apps
 7. CLI

One reason to send authenticated requests is to increase the rate limit imposed
by GitHub. Unauthenticated  requests are limited to 60 per  hour, and are based
upon IP address.  Authenticated requests are limited to 5000 per hour.

Sending an anonymous request responds with =X-RateLimit-*= headers stating that
less than 60 more requests can be sent until the rate limit is reset.

*** GitHub API Authentication---Personal Access Tokens
    :PROPERTIES:
    :uri:      https://dev.to/gr2m/github-api-authentication-personal-access-tokens-53kd
    :END:
You can create a new Personal Access Token at:
- https://github.com/settings/tokens/new

Fill in the form and then move to the next screen, which will show you the
token.  Copy it somewhere safe; you will never be able to see it again.

You can now use the token from the command line with ~curl~:
: curl --header "Authorization: token d64761df071c2bf517ceb063b279432ed2f89c62" \
:     https://api.github.com/repos/octokit/core.js/releases/latest

Or with ~fetch~ in a browser or ~node-fetch~ in Node.js:

#+begin_src js
  const response = await fetch(
    "https://api.github.com/repos/octokit/core.js/releases/latest", 
    {
      headers: {
	authorization: "token d64761df071c2bf517ceb063b279432ed2f89c62"
      }
    }
  )
  console.log(await response.json());
#+end_src

**** Use the JavaScript OctoKit
Authenticating  using a  personal access  token  is straight  forward, so  it's
already built  into https://github.com/octokit/core.js  and all  libraries that
are built upon it.

Sending the above request would look like this in the browser

#+begin_src html
  <script type="module">
  import { Octokit } from "https://cdn.pika.dev/@octokit/core";

  const octokit = new Octokit({ auth: "d64761df071c2bf517ceb063b279432ed2f89c62" });
  octokit.request('GET /repos/:owner/:repo/releases/latest', {
    owner: "octokit",
    repo: "core.js"
  }).then(response => console.log(response.data))
  </script>
#+end_src

And like this in Node.js

#+begin_src js
  const { Octokit } = require('@octokit/rest')
  const octokit = new Octokit({ auth: "d64761df071c2bf517ceb063b279432ed2f89c62" });
  octokit.request('GET /repos/:owner/:repo/releases/latest', {
    owner: "octokit",
    repo: "core.js"
  }).then(response => console.log(response.data))
#+end_src

**** Handling Errors
If the token is  invalid, the server will respond with a 401  status and a "bad
credentials" message.

#+begin_src sh
  curl --header "Authorization: token invalid" https://api.github.com/notifications
  {
    "message": "Bad credentials",
    "documentation_url": "https://developer.github.com/v3"
  }
#+end_src

If the token does not have the  required scopes, the server will respond with a
403 status and an explanatory message.

#+begin_src sh
  curl --header "Authorization: token d64761df071c2bf517ceb063b279432ed2f89c62" \
       https://api.github.com/notifications
  {
    "message": "Missing the 'notifications' scope.",
    "documentation_url": "https://developer.github.com/v3/activity/notifications/#list-your-notifications"
  }
#+end_src

New scopes cannot  be added to existing  tokens, you will have to  create a new
token with the required scopes selected to address 403 errors.

**** Limitations
Personal Access Tokens work great for personal usage. But if you plan to create
a service  or a CLI  application that integrate  with GitHub, there  are better
options that don't require the user to manually create and maintain tokens.

Personal Access Tokens can be used in GitHub Actions if you want the script to
act as your user account.

*** GitHub API Authentication---GitHub Actions
    :PROPERTIES:
    :uri:      https://dev.to/gr2m/github-api-authentication-github-actions-la3
    :END:
In this post, I will explain how to create a GitHub Action workflow that adds a
comment to every new pull request using

1. A GitHub Action workflow file: ~.github/workflows/pr-comment.yml~
2. A single JavaScript file: ~.github/actions/pr-comment.js~

Both files live in the same repository where the comments will be added.

-  here is [[https://github.com/gr2m/create-pull-request-comment-action-example][the repository]] with all the code described in this post.

You will need:
1. ~git~ and Node.js installed
2. A repository on GitHub
3. the repository cloned to your local machine

**** Passing the GITHUB_TOKEN secret to a JavaScript file
GitHub Actions come  with their own special  token that must be  passed to each
workflow step explicitly: =secrets.GITHUB_TOKEN=. Unlike Personal Access Tokens
that  I explained  in the  previous  post of  this  series, you  don't have  to
manually create them. A unique =GITHUB_TOKEN=  is created each time your GitHub
Action is run.

Let's    start    out     by    creating    a    new     workflow    file    at
~.github/workflows/pr-comment.yml~

#+name: .github-workflows-pr-comment-yml
#+begin_example
  name: PR Comment
  on:
    # Run this workflow only when a new pull request is opened
    # compare: https://git.io/JvTyV
    pull_request:
      types: [opened]

  jobs:
    pr_comment:
      runs-on: ubuntu-latest

      steps:
	# Make files accessible to actions
	# https://github.com/actions/checkout#readme
	- uses: actions/checkout@v2
	# Install Node
	# https://github.com/actions/setup-node#readme
	- uses: actions/setup-node@v1
	  with:
	    node-version: 12
	# Install dependencies
	- run: npm ci
	# Run pr-comment.js with Node and pass the authentication token 
	- run: node .github/actions/pr-comment.js
	  with:
	    GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
#+end_example

**** Creating a comment using JavaScript
We will use GitHub's =Create a comment= REST API endpoint to create the comment
on every  new pull  request using ~octokit.request~.

1. Create a ~package.json~ file in the folder you cloned your repository into:
: npm init

2. After that, install ~@octokit/action~
: npm install @octokit/action

3. Next, create the ~.github/actions/pr-comment.js~ file

#+name: .github-actions-pr-comment-js
#+begin_src js
  // GITHUB_EVENT_PATH always exists when run by an Action,
  // see https://git.io/JvUf7 for a full list
  const eventPayload = require(process.env.GITHUB_EVENT_PATH);
  const { Octokit } = require("@octokit/action");

  createPrComment();

  async function createPrComment() {
    // No need to pass process.env.GITHUB_TOKEN, `@octokit/action`
    // is using it directly and throws an error if it is not present.
    const octokit = new Octokit();

    // See https://developer.github.com/v3/issues/comments/#create-a-comment
    const { data } = await octokit.request(
      "POST /repos/:repository/issues/:pr_number/comments",
      {
	repository: process.env.GITHUB_REPOSITORY,
	pr_number: eventPayload.pull_request.number,
	body: "Thank you for your pull request!"
      }
    );

    console.log("Comment created: %d", data.html_url);
  }
#+end_src

4. Commit and push your changes

: git add .
: git commit -m 'add GitHub Action workflow to comment on new PRs'
: git push origin master

5. Now create a  new pull request on your repository. After  a short delay, the
   Action will show up in the list of checks as pending

**** Secrets and pull requests from forks
Creating the comment will  not work when someone creates a  pull request from a
fork. The =GITHUB_TOKEN= secret is still passed, but has only read permissions,
it cannot  create or update  anything. If that was  not the case,  anyone could
create a  pull request changing  the code of  the ~pr-comment.js~ script  to do
something malicious with your repository.

For now,  I you  can prevent  the action  from running  altogether if  the pull
request comes from a fork by adding an if statement

#+begin_example
  # ...

  jobs:
    pr_comment:
      runs-on: ubuntu-latest
      if: eventPayload.pull_request.head.repo.fork == false
      steps:
	# ...
#+end_example

Another alternative  is to use  GitHub Apps instead, which  I will cover  in my
next blog post.

*** GitHub API Authentication---Username and Password--Basic
    :PROPERTIES:
    :uri:      https://dev.to/gr2m/github-api-authentication-username-password-basic-4na5
    :date:     2020-02-14
    :END:

Today,  on February  14, 2020,  GitHub announced  its deprecation  timeline for
authenticating using a username and a  password. Which means you have only time
until November 13, 2020 to give this a try ü§™.

I have  mixed feelings about the  deprecation. It makes me  happy because Basic
authentication  has   all  kinds   of  security  problems.   Adding  two-factor
authentication made it a bit more secure, but also a pain in the üçë.

I'm sad,  because I  created ~@octokit/auth-basic~!  to hide  away most  of the
complexities introduced by two-factor authentication, and I think it turned out
pretty nicely üò≠ I think it's a good  example of what an API client library can
do to hide away complexities from consumers of that API.

So,  for the  history  books, let's  see how  to  use ~@octokit/auth-basic~  to
authenticate using =username=, =password=, and two-factor authentication.

**** How Basic authentication works for the GitHub API
Let's  try  to   send  a  request  to  GitHub's  =GET   /user=  API  using  the
~@octokit/request~ package.

#+begin_src js
  // my-cli.js
  const { request } = require("@octokit/request");

  const USERNAME = "octocat";
  const PASSWORD = "secret";

  request("GET /user", {
    headers: {
      authorization: `basic ${Buffer.from(`${USERNAME}:${PASSWORD}`).toString(
	"base64"
      )}`
    }
  }).then(response => console.log(response.data), console.error);
#+end_src

Depending on your GitHub security Settings,  the above code will either log the
user object for your account, or it  will fail with a 401 response, including a
'X-GitHub-Otp' header with the value set to =required; app= or =required; sms=.

In order to retrieve your user account,  you will need to send the same request
again, including a header containing the OTP. OTP stands for one-time password.
In GitHub's case, you can use the OTP  more than once, because it is actually a
time-based password. It usually is valid for about a minute. ü§∑.

If you use an  authenticator app (you should!), you already  know the right OTP
to send along, the request code looks like this:

#+begin_src js
  // my-cli.js
  const { request } = require("@octokit/request");

  const USERNAME = "octocat";
  const PASSWORD = "secret";
  const OTP = "012345";

  request("GET /user", {
    headers: {
      authorization: `basic ${Buffer.from(`${USERNAME}:${PASSWORD}`).toString(
	"base64"
      )}`,
      "x-github-otp": OTP
    }
  }).then(response => console.log(response.data), console.error);
#+end_src

If you  have SMS setup  for your  two-factor authentication (you  should not!),
then you are out of  luck. Not only do you not know the OTP  at the time :of the
first request,  you won't  even receive an  SMS with an  OTP from  GitHub. Why?
Because  only certain  REST  API routes  trigger the  SMS  delivery. The  OAuth
Authorizations API routes, to be precise.

In order to workaround this problem, the  recommend best practice is to not use
basic authentication  for every request. Instead,  use it to create  a personal
access token, then use that token for the following requests.

And because  you create a  personal access token  that you probably  won't need
ever  again, it's  a good  practice to  delete that  token when  you are  done.
However, the OTP  you used to create  the token might no longer  be valid (time
based, remember),  so it's well  possible that GitHub  will respond with  a 401
asking for a new OTP.

You can see, this is getting complicated pretty quick, and it's only the tip of
the ice berg. For example, some requests require to be authenticated using your
username and password, while for most others  you can use the token. If you are
curious, you  can read trough the  source code of [[https://github.com/octokit/auth-basic.js][@octokit/auth-basic]]  to learn
all about it. The tests will give you a pretty good summary.

**** @octokit/basic-auth
[[https://github.com/octokit/auth-basic.js][@octokit/basic-auth]]  takes  away most  of  the  pain  that  is Basic  Auth  and
two-factor authentication for GitHub's REST API. It even integrates neatly with
your favorite  Octokit libraries such  as [[https://octokit.github.io/rest.js/#authentication][@octokit/rest]], [[https://github.com/octokit/core.js#authentication][@octokit/core]]  or even
the super low-level [[https://github.com/octokit/request.js#authentication][@octokit/request]].

In   this   example   I'll  use   ~@octokit/basic-auth~,   ~@octokit/request~   and
~readline-sync~:

#+begin_src js
  // my-cli.js
  const { createBasicAuth } = require("@octokit/auth-basic");
  const { request } = require("@octokit/request");
  const { question } = require("readline-sync");

  const auth = createBasicAuth({
    username: question("Username: "),
    password: question("Password: "),
    async on2Fa() {
      // prompt user for the one-time password retrieved via SMS or authenticator app
      return question("Two-factor authentication Code: ");
    }
  });

  const requestWithBasicAuth = request.defaults({
    request: {
      hook: auth.hook
    }
  });

  requestWithBasicAuth("GET /user").then(
    response => console.log(response.data),
    console.error
  );
#+end_src

When you run the  above code with Node, you will be  prompted for your username
and  password.  If you  have  two-factor  auth  setup  and SMS  configured  for
delivery, you  will receive an  SMS with  the OTP. Once  you enter the  OTP the
script will log the user object for your GitHub Account to your terminal.

Now lets  say you need to  send so many  requests that the OTP  becomes invalid
(usually about  a minute),  but you  still want to  delete the  personal access
token at the end. The code would look something like this:

#+begin_src js
  // my-cli.js
  const { createBasicAuth } = require("@octokit/auth-basic");
  const { request } = require("@octokit/request");
  const { question } = require("readline-sync");

  run();

  async function run() {
    const auth = createBasicAuth({
      username: question("Username: "),
      password: question("Password: "),
      async on2Fa() {
	// prompt user for the one-time password retrieved via SMS or authenticator app
	return question("Two-factor authentication Code: ");
      }
    });

    const requestWithBasicAuth = request.defaults({
      request: {
	hook: auth.hook
      }
    });

    const { data } = await requestWithBasicAuth("GET /user");
    console.log(`Your GitHub Account ID: ${data.id}`);

    console.log(`Sending some more requests that take a while ...`);
    const TWO_MINUTES_IN_MS = 2 * 60 * 1000;
    await new Promise(resolve => setTimeout(resolve, TWO_MINUTES_IN_MS));

    const { id } = await auth({ type: "token" });
    await requestWithBasicAuth("DELETE /authorizations/:authorization_id", {
      authorization_id: id
    });
    console.log("TOKEN deleted");
  }
#+end_src

The code above has  a two minute timeout build in to make  sure the OTP becomes
invalid. You will see that you will get prompted for an OTP for the 2nd time:

#+begin_src sh
  $ node my-cli.js
  Username: gr2m
  Password: ***
  Two-factor authentication Code: 068194
  Your GitHub Account ID: 39992
  Sending some more requests that take a while ...
  Two-factor authentication Code: 975808
  TOKEN deleted
#+end_src

**** What are the alternatives to Basic authentication
Well,  the  Basic authentication  party  is  over soon,  so  make  sure to  use
alternative means of authentication before November 2020.

You can do one of two things.

1. Ask your users to create a personal access token and share that with you.
2. Use GitHub's [[https://developer.github.com/apps/building-oauth-apps/authorizing-oauth-apps/#web-application-flow][OAuth web application flow]].


Now 2. is a  nicer user experience, but it's easier said  that done. Luckily, I
have two blog posts lined up that will help you:

1. OAuth: How to implement the OAuth web flow using a server and a client
2. CLI: How to use the OAuth web flow for CLI apps.
** Building OAuth Apps on GitHub
   :PROPERTIES:
   :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/building-oauth-apps
   :END:
Learn how  to register  and set  up permissions  and authorization  options for
OAuth Apps.
*** Create an OAuth App
    :PROPERTIES:
    :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/creating-an-oauth-app
    :END:
You can create and  register an OAuth App under your  personal account or under
any organization you have administrative access to.

1. Settings
2. Developer Settings
3. OAuth Apps
4. New OAuth App
5. Application name
6. Homepage URL
7. Description
8. Authorization Callback URL
9. Register application

*** Authorize an OAuth App
    :PROPERTIES:
    :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/authorizing-oauth-apps
    :END:
You can enable other users to authorize your OAuth App.

GitHub's OAuth implementation supports:
- the standard =authorization code grant= type
- the OAuth 2.0 =Device Authorization Grant= for apps that don't have access to
  a web browser.
- the non-web application flow.

To authorize your  OAuth app, consider which authorization flow  best fits your
app.

- web application flow ::

  Used to authorize users for standard OAuth apps that run in the browser. (The
  implicit grant type is not supported.)

- device flow ::

  Used for headless apps, such as CLI tools.

**** Web Application Flow
     :PROPERTIES:
     :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/authorizing-oauth-apps#web-application-flow
     :END:
  The web application flow to authorize users for your app is:
   1. Users are redirected to request their GitHub identity
   2. Users are redirected back to your site by GitHub
   3. Your app accesses the API with the user's access token

***** Request a user's GitHub identity
  : GET https://github.com/login/oauth/authorize

  When your  GitHub App specifies  a =login= parameter,  it prompts users  with a
  specific account they can use for signing in and authorizing your app.

****** Parameters
   - client_id :: (string) *Required* The client ID you received from GitHub when
     you registered.
   - redirect_uri ::  (string) The URL  in your  application where users  will be
     sent after authorization.
   - login ::  (string) Suggests  a specific  account to use  for signing  in and
     authorizing the app.
   - scope :: (string)  A space-delimited list of scopes. If  not provided, scope
     defaults to an empty list for users  that have not authorized any scopes for
     the application.
   - state :: (string) An unguessable random string. It is used to protect
     against cross-site request forgery attacks.
   - allow_signup  ::  (string) Whether  or  not  unauthenticated users  will  be
     offered an option to  sign up for GitHub during the  OAuth flow. The default
     is =true=.

***** Users are redirected back to your site by GitHub
  If the  user accepts your  request, GitHub redirects back  to your site  with a
  temporary code  in a code parameter  as well as  the state you provided  in the
  previous step  in a state  parameter. The temporary  code will expire  after 10
  minutes. If the states don't match, then a third party created the request, and
  you should abort the process.

  Exchange this code for an access token:

  : POST https://github.com/login/oauth/access_token

****** Parameters
   - client_id :: (string) *Required* The client  ID you received from GitHub for
     your GitHub App.
   - client_secret ::  (string) *Required*  The client  secret you  received from
     GitHub for your GitHub App.
   - code :: (string) *Required* The code you received as a response to Step 1.
   - redirect_url ::  (string) The URL in  your application where users  are sent
     after authorization.
   - state :: (string) The unguessable random string you provided in Step 1.

****** Response
  By default, the response takes the following form:

  : access_token=e72e16c7e42f292c6912e7710c838347ae178b4a&token_type=bearer

  You can also receive the content in different formats depending on the =Accept=
  header:

  #+begin_example
    Accept: application/json
    {"access_token":"e72e16c7e42f292c6912e7710c838347ae178b4a", "scope":"repo,gist", "token_type":"bearer"}

    Accept: application/xml
    <OAuth>
      <token_type>bearer</token_type>
      <scope>repo,gist</scope>
      <access_token>e72e16c7e42f292c6912e7710c838347ae178b4a</access_token>
    </OAuth>
  #+end_example

***** Use the access token to access the API
  The access token allows you to make requests to the API on a behalf of a user.

  : Authorization: token OAUTH-TOKEN
  : GET https://api.github.com/user

  For example, in curl you can set the Authorization header like this:

  : curl -H "Authorization: token OAUTH-TOKEN" https://api.github.com/user
**** Device Flow
The device flow allows you to authorize users for a headless app, such as a CLI
tool or Git credential manager.

Overview of the device flow:

 1. Your  app  requests  device  and user  verification  codes  and  gets  the
   authorization URL where the user will enter the user verification code.

 2. The app prompts the user to enter a user verification code at:
    : https://github.com/login/device.

 3.  The app  polls  for the  user  authentication status.  Once  the user  has
    authorized the device,  the app will be  able to make API calls  with a new
    access token.

***** Step 1---App Requests the Device and User Verification Codes from GitHub
: POST https://github.com/login/device/code

Your app  must request a user  verification code and verification  URL that the
app will use to prompt the user  to authenticate in the next step. This request
also returns  a device verification  code that the app  must use to  receive an
access token and check the status of user authentication.

****** Input Parameters
 - client_id :: (string) *Requirerd* The client ID you received from GitHub for
   your app.
 - scope :: (string) The scope that your app is requesting access to.

****** Response Parameters
#+begin_src js
  {
    "device_code": "3584d83530557fdd1f46af8289938c8ef79f9dc5",
    "user_code": "WDJB-MJHT",
    "verification_uri": "https://github.com/login/device",
    "expires_in": 900,
    "interval": 5
  }
#+end_src

 - device_code ::  (string) The device  verification code is 40  characters and
   used to verify the device.
 - user_code :: (string) The user verification  code is displayed on the device
   so the user can enter the code in  a browser. This code is 8 characters with
   a hyphen in the middle.
 - verification_url  :: The  verification URL  where  users need  to enter  the
   user_code:
   : https://github.com/login/device
 - expires_in ::  (integer) The  number of seconds  before the  device_code and
   user_code expire. The default is 900 seconds or 15 minutes.
 - interval ::  (integer) The minimum number  of seconds that must  pass before
   you     can     make     a     new    access     token     request     (POST
   https://github.com/login/oauth/access_token)   to    complete   the   device
   authorization.

***** Step 2---Prompt the user to enter the user code in a browser
Your device will show  the user verification code and prompt  the user to enter
the code at
: https://github.com/login/device.

***** Step 3---App polls GitHub to check if the user authorized the device
Your app will make device authorization requests that poll
: POST https://github.com/login/oauth/access_token
until the device and user codes  expire or the user has successfully authorized
the app with a  valid user code. The app must use  the minimum polling interval
retrieved in step 1 to avoid rate limit errors.

The user must enter  a valid code within 15 minutes (or  900 seconds). After 15
minutes, you will need to request a new device authorization code with
: POST https://github.com/login/device/code

Once the user has authorized, the app  will receive an access token that can be
used to make requests to the API on behalf of a user.

****** Input Parameters
 - client_id :: (string) *Required* The client  ID you received from GitHub for
   your OAuth App.
 - device_code :: (string) *Required* The device verification code you received
   from the request to:
   : POST https://github.com/login/device/code
 - grant_type :: (string) *Required* The grant type must be:
   : urn:ietf:params:oauth:grant-type:device_code

****** Response
#+begin_src js
  {
   "access_token": "e72e16c7e42f292c6912e7710c838347ae178b4a",
    "token_type": "bearer",
    "scope": "user"
  }
#+end_src

***** Error Codes for the Device Flow
      :PROPERTIES:
      :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/authorizing-oauth-apps#error-codes-for-the-device-flow
      :END:
 - authorization_pending
 - slow_down
 - expired_token
 - unsupported_grant_type
 - incorrect_client_credentials
 - incorrect_device_code
 - access_denied

**** Non-Web Application Flow

**** Redirect URLs

**** Creating Multiple Tokens for OAuth Apps

**** Directing Users to Review Their Access

*** Set Scopes for an OAuth App
    :PROPERTIES:
    :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/scopes-for-oauth-apps
    :END:
*** Create a Custom Badge for an OAuth App
    :PROPERTIES:
    :uri:      https://docs.github.com/en/free-pro-team@latest/developers/apps/creating-a-custom-badge-for-your-oauth-app
    :END:

* Build Tools
:PROPERTIES:
:appendix: t
:custom_id: build-tools
:END:
** Makefile					:dependencies:env_vars:perl:
:PROPERTIES:
:appendix: t
:dependency1: make
:dependency2.0: AWS User account at https://aws.amazon.com
:dependency2.1: AWS cli v2 in PATH https://docs.aws.amazon.com/cli/index.html
:dependency2.2: See how to Install AWS CLI v2 at https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2-mac.html
:dependency2.3: aws credentials: access token and secret access token stored in ~/.aws/credentials
:dependency2.4: AWS S3 buckets set up for serving a static web page
:dependency3: GitHub Account with personal access token stored in GITHUB_TOKEN
:dependency4: texinfo @6.7._
:dependency5: Emacs, Org-mode, Babel language 'shell' enabled
:env_var1: SYNC_ORG_TEMPLATE: holds the full path to this Template.org file
:env_var2: GITHUB_TOKEN: holds the GitHub personal access token
:env_var3: EDITOR: must hold a reference to a working emacsclient server
:env_var4: COLORS
:END:

#+pindex:Makefile
#+name:Makefile
#+header: :tangle Makefile
#+begin_src makefile

  ###############################################################################
  ### USER-DEPENDENT VARIABLES
  ### USE ENVIRONMENT VARIABLES WHENEVER POSSIBLE

  # NOTE: All environment variables need to be exported PRIOR to starting the
  # Emacs server as EDITOR in your shell startup files; otherwise, they will not
  # be available to Emacs.
  # When I moved from using Bash to Zsh, I inadvertently changed the order of
  # import, and started the Emacs server before importing, and caused a horrible
  # bug which caused the program to work on one computer but fail on another.

  # The absolute path to this Template file
  TEMPLATE := $(SYNC_ORG_TEMPLATE)


  ### TOOLS & RESOURCES
  # tools is a directory holding tangled scripts, such as cmprpl
  # resources is a directory holding static resources for the project
  # images is a directory holding jpg and png image files
  RESOURCES := resources
  TOOLS	    := $(RESOURCES)/tools
  IMAGES    := $(RESOURCES)/images
  CMPRPL    := $(TOOLS)/cmprpl

  # Use emacsclient as $EDITOR; make sure it is set in a shell startup file and
  # the server has been started.
  EMACS	  := $(EMACS)
  EDITOR  := $(EDITOR)

  # User‚Äôs personal GitHub token for authentication to GitHub
  # DO NOT HARD-CODE THIS VALUE
  GITHUB_TOKEN := $(GITHUB_TOKEN)

  # The AWS Command Line Interface (AWS CLI) is an open source tool
  # that enables you to interact with AWS services using commands in
  # your command-line shell.  It must be present on your system.  Run the 'make'
  # command 'install-aws-cli' to install it if you do not have it.  Be sure to
  # run 'aws configure' after installing it.  This will place your AWS
  # credentials into ~/.aws/credentials.
  AWS := aws
  S3  := $(AWS) s3
  CFD := $(AWS) cloudfront

  ### END OF USER-DEPENDENT VARIABLES
  ###############################################################################
  ### MAKE-GENERATED VARIABLES

  ### PROJ AND ORG
  # ORG is the name of this Org file with extension .org
  # PROJ is the project name---the Org file name without extension.

  ### NOTE: there can be only one Org file in the project directory;
  # so far this has not been a problem, but it might be.

  PWD  := $(shell pwd)
  ORG  := $(shell ls *.org)
  PROJ := $(basename $(ORG))

  ### NOTE: S is needed only for the Template file because of the way it is nested
  # one level deep in the Templates GitHub repo, which uses the plural form
  # of Templates, whereas this file uses the singular form, Template.  So when
  # the homepage link is updated, the curl command must be told to use the plural
  # form.	 This is obviously a hack only for my own use and can be removed once
  # I clean up this anomaly.

  ifeq ($(PROJ),$(basename $(notdir $(TEMPLATE))))
  S := s
  endif

  # The AWS S3 bucket to use to store the html source file; it is found at the
  # key #+bucket towards the beginning of the file and should include the appropriate
  # suffix (.com, .net, .org, etc)
  BUCKET       := $(shell $(EDITOR) --eval \
		 '(with-current-buffer (find-file-noselect "$(ORG)") \
		    (save-excursion \
		      (goto-char (point-min)) \
		      (re-search-forward "^\#[+]bucket:\\(.*\\)$$" nil t) \
		      (match-string-no-properties 1)))')
  S3_BUCKET    := s3://$(BUCKET)

  # Buckets set up to serve static web sites from S3 can use either http
  # or https protocols; some  http protocols will automatically redirect
  # to https;  however, some only use  http. I would like  to accomodate
  # both, and  so this code  finds the url's  that are in  my Cloudfront
  # account, which presumably will serve https.  If the url is not here,
  # then this must be set up to serve http instead.
  HTTP_S := $(shell $(CFD) list-distributions | perl -MJSON::PP -e \
	  '$$/=""; \
	   my @urls = (); \
	   my $$json=JSON::PP->new->decode(<STDIN>); \
	   for my $$item ( @{$$json->{"DistributionList"}{"Items"}} ) { \
		  push @urls, @{$$item->{"Aliases"}{"Items"}}; \
	   } \
	  my $$found = grep { /'$(BUCKET)'/ } @urls; \
	  print "http", ($$found ? "s" : "");')

  HTTPS_BUCKET := https://$(BUCKET)

  ### DIR, SRC
  # DIR is the .info name found at '#+texinfo_filename:<DIR>.info' (at
  # the bottom of this file in the export configuration settings)
  # without its extension, used as the INFO filename and the name of the
  # HTML export directory; this code uses the lowercased PROJ name if
  # there is no '#+texinfo_filename'.
  # SRC is HTML directory based upon the DIR name

  #DIR := $(shell $(EDITOR) --eval \
  #	'(with-current-buffer (find-file-noselect "$(ORG)") \
  #		(save-excursion \
  #		(goto-char (point-min)) \
  #		(re-search-forward "^\#[+]\\(?:texinfo_filename\\|TEXINFO_FILENAME\\):\\(.*\\).info$$" nil t) \
  #		(match-string-no-properties 1)))')

  DIR := $(shell sed -E -n "/^\#\+texinfo_filename/s/^.*:(.*)\.info$$/\1/p" $(ORG))
  ifeq ($(DIR),$(EMPTY))
	  DIR := $(shell echo $(PROJ) | tr "[:upper:]" "[:lower:]")
  endif

  SRC := $(DIR)/

  ### VERS: v1.2.34/
  # VERS is the version number of this Org document.
  # When sync is run after the version number has been updated, then VERS
  # picks up the newly-changed value.  VERS used to be staticly imbedded
  # when the Makefile was tangled, but it needs to be dynamic for
  # development.

  # QUERY: should this number be formatted like this, or should it be just the numbers?
  # The reason it includes them is the S3PROJ obtains the name from the S3 bucket, and
  # it includes them.  But it only includes them because I have made it so.  Not a good
  # reason just by itself.  The ending slash is not actually a part of the version, but
  # comes from the way the 'aws2 ls' command returns its values.	So VERS should probably
  # not include the trailing slash, although it doesn‚Äôt hurt anything.

  VERS := v$(shell $(EDITOR) --eval \
	  '(with-current-buffer (find-file-noselect "$(ORG)") \
		  (save-excursion \
		    (goto-char (point-min)) \
		    (re-search-forward "^\#[+]\\(?:macro\\|MACRO\\):version Version \\(\\(?:[[:digit:]]+[.]?\\)\\{3\\}\\)") \
		    (match-string-no-properties 1)))')/

  ### AWS
  # PROJ_LIST contains the list of projects currently uploaded to
  # the S3 bucket; each item contains the name of the project and its
  # current version.

  # Created function using elisp instead of the shell.
  # This variable contains an elisp list of strings of the form '("proj1-v1.2.3/" "proj2-v4.5.6/" ...)'
  # However, when it prints to the shell, the quotes are lost.
  # Need to make sure elisp's variable 'exec-path contains the proper $PATH instead of adding to 'exec-path.

  PROJ_LIST := $(shell $(EDITOR) --eval \
	  "(progn \
		  (require (quote seq)) (add-to-list (quote exec-path) (quote \"/usr/local/bin\")) \
		  (seq-map (lambda (s) (replace-regexp-in-string \"^\s+PRE \" \"\" s)) \
			  (seq-filter (lambda (s) (string-match-p (regexp-quote \" PRE \") s)) \
			  (process-lines \"$(AWS)\" \"s3\" \"ls\" \"$(S3_BUCKET)\"))))")

  ### S3PROJ
  # The name of the current project as obtained from S3: 'proj-v1.2.34/'
  # If there is no current project in the S3 bucket, then assign a value equal to
  # the Org project and version instead.  It is set to the project if found, and
  # NO if not found, then updated in the ifeq block below.
  S3PROJ := $(shell $(EDITOR) --eval \
		  '(let ((proj (seq-find (lambda (s) (string-match-p "$(DIR)" s)) (quote $(PROJ_LIST))))) \
		     (or proj (quote NO)))')

  ### PROJINS3
  # is used by make sync; this allows the index.html file to be generated the first
  # time the project is synced.  It is set to NO if this project is not currently in an
  # S3 bucket, and it is set to YES if it is.
  PROJINS3 :=

  ### S3VERS
  # The version of this project currently installed in the S3 bucket: 'v1.2.34/'
  # If there is no current version in the S3 bucket, then assign the version from
  # this Org file instead.
  S3VERS   :=

  # Update S3PROJ, S3VERS, and PROJINS3
  ifeq ($(S3PROJ), NO)
	  S3PROJ := $(DIR)-$(VERS)
	  S3VERS := $(VERS)
	  PROJINS3 := NO
  else
	  S3VERS := $(subst $(DIR)-,,$(S3PROJ))
	  PROJINS3 := YES
  endif

  ### GITHUB
  # USER is the current user's GitHub login name.

  # The user name used to be statically embedded into the Makefile
  # during tangle, but in an effort to make the Makefile dynamically
  # indepedent, dynamic code has replaced the static code.  The code
  # that placed the static name in the Makefile was a 'node' script that
  # ran in a separate Org process during tangle.	An unfortunate fact of
  # 'make' is that 'make' strips the quote marks from the string
  # obtained from the 'curl' command when the 'make shell' command
  # returns the string.	 This makes the string malformed JSON and
  # unparsable by most JSON parsers, including 'node‚Äô.	However,
  # 'perl'‚Äôs core module JSON::PP (but not JSON::XS) has facilities to
  # parse very malformed JSON strings.	Therefore, this dynamic code
  # uses 'perl' and the core module JSON::PP to parse the 'curl' string
  # into a 'perl' JSON object which can return the login name.	This
  # code should work with any version of 'perl' without having to
  # install any modules.

  USER	:= $(shell \
	    curl -sH "Authorization: token $(GITHUB_TOKEN)" https://api.github.com/user \
	    | \
	    perl -MJSON::PP -e \
		'$$/ = ""; \
		 my $$json = JSON::PP->new->loose->allow_barekey->decode(<STDIN>); \
		 print $$json->{login};' \
	    )
  SAVE		:= resources

  ### TEXINFO
  TEXI		:= $(PROJ).texi
  INFO		:= $(DIR).info
  INFOTN	:= $(shell $(EDITOR) --eval "(file-truename \"$(INFO)\")")
  PDF		:= $(PROJ).pdf
  INDEX		:= index.html
  HTML		:= $(DIR)/$(INDEX)
  DIR_OLD	:= $(DIR)-old

  ### AWS S3
  DST_OLD	:= $(S3_BUCKET)/$(S3PROJ)
  DST_NEW	:= $(S3_BUCKET)/$(DIR)-$(VERS)
  EXCL_INCL	:= --exclude "*" --include "*.html"
  INCL_IMAGES	:= --exclude "*" --include "*.jpg" --include "*.png"
  GRANTS	:= --grants read=uri=http://acs.amazonaws.com/groups/global/AllUsers
  S3SYNC	:= $(S3) sync --delete $(EXCL_INCL) $(SRC) $(DST_OLD) $(GRANTS)
  S3MOVE	:= $(S3) mv --recursive $(DST_OLD) $(DST_NEW) $(GRANTS)
  S3COPY	:= $(S3) cp $(INDEX) $(S3_BUCKET) $(GRANTS)
  S3REMOVE	:= $(S3) rm $(S3_BUCKET)/$(S3PROJ) --recursive
  S3IMAGESYNC	:= $(S3) sync $(INCL_IMAGES) $(IMAGES) $(S3_BUCKET)/$(IMAGES) $(GRANTS)

  ###############################################################################

  default: check texi info html pdf

  PHONY: default all check values boot \
	    texi info html pdf \
	    open-org open-texi open-html open-pdf \
	    clean dist-clean wiped-clean \
	    help sync update delete-proj \
	    install-aws-cli \
	    index-html upload-index-html

  values: check
	    @printf "$${BLUE}Values...$${CLEAR}\n"
	    @echo TEMPLATE:	$(TEMPLATE)
	    @echo EDITOR:	$(EDITOR)
	    @echo USER:		$(USER)
	    @echo PWD:		$(PWD)
	    @echo ORG:		$(ORG)
	    @echo TEXI:		$(TEXI)
	    @echo INFO:		$(INFO)
	    @ECHO INFOTN:	$(INFOTN)
	    @echo BUCKET:	$(BUCKET)
	    @echo PROJ:		$(PROJ) $S
	    @echo S3_BUCKET:	$(S3_BUCKET)
	    @echo HTTP_S:	$(HTTP_S)
	    @echo HTTPS_BUCKET:	$(HTTPS_BUCKET)
	    @echo VERS:		$(VERS)
	    @echo S3PROJ:	$(S3PROJ)
	    @echo S3VERS:	$(S3VERS)
	    @echo DIR:		$(DIR)
	    @echo DIR_OLD:	$(DIR_OLD)
	    @echo SRC:		$(SRC)
	    @echo DST_OLD:	$(DST_OLD)
	    @echo DST_NEW:	$(DST_NEW)
	    @echo PROJ_LIST:	"$(PROJ_LIST)"
	    @echo PROJINS3:	$(PROJINS3)

  check:
	    @printf "$${BLUE}Checking dependencies...$${CLEAR}\n"

	    @[[ -z $(BUCKET) ]] && \
	       { printf "$${RED}$(BUCKET) $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}BUCKET: $${GREEN}$(BUCKET)$${CLEAR}\n";

	    @[[ -z $${GITHUB_TOKEN} ]] && \
	       { printf "$${RED}GITHUB_TOKEN $${CYAN}must be set.$${CLEAR}\n"; exit 1; } || \
	       printf "$${CYAN}GITHUB_TOKEN: $${GREEN}SET$${CLEAR}\n";

	    @[[ (-d ~/.aws) && (-f ~/.aws/credentials) && (-f ~/.aws/config) ]] && \
	       printf "$${CYAN}AWS credentials and config: $${GREEN}SET$${CLEAR}\n" || \
	       { printf "$${RED}~/.aws 'credentials' and 'config' must be set.$${CLEAR}\n"; exit 1; }

	    @[[ "$(shell $(EDITOR) --eval '(member (quote texinfo) org-export-backends)')" = "(texinfo)" ]] && \
		  printf "$${CYAN}Texinfo backend: $${GREEN}INSTALLED.$${CLEAR}\n" || \
		  { printf "$${YELLOW}Texinfo backend:$${CLEAR} $${RED}NOT INSTALLED; it must be installed.$${CLEAR}\n"; exit 1; }

	    @[[ $(shell $(EDITOR) --eval '(symbol-value org-confirm-babel-evaluate)') == "t" ]] && \
		  { printf "$${YELLOW}org-confirm-babel-evaluate:$${CLEAR} $${RED}T; set to NIL.$${CLEAR}\n"; exit 1; } || \
		  printf "$${CYAN}org-confirm-babel-evaluate: $${GREEN}OFF.$${CLEAR}\n\n"

  open-org: $(ORG)
	    @$(EDITOR) -n $(ORG)
  $(ORG):
	    @echo 'THERE IS NO $(ORG) FILE!!!'
	    exit 1

  texi: $(TEXI)
  $(TEXI): $(ORG)
	   @echo Making TEXI...
	   @$(EDITOR) -u --eval \
		  "(with-current-buffer (find-file-noselect \"$(ORG)\" t) \
			  (save-excursion \
			  (org-texinfo-export-to-texinfo)))"
	   @echo Done making TEXI.
  open-texi: texi
	   @$(EDITOR) -n $(TEXI)

  info: $(INFO)
  $(INFO): $(TEXI)
	   @echo Making INFO...
	   @makeinfo -o $(INFO) $(TEXI)
	   @$(EDITOR) -u -eval \
		  "(when (get-buffer \"$(INFO)\") \
			  (with-current-buffer (get-buffer \"$(INFO)\") \
				  (revert-buffer t t t)))"
	   @echo Done making INFO.

  open-info: info
	   @$(EDITOR) -u -eval \
		  "(if (get-buffer \"*info*\") \
			  (with-current-buffer (get-buffer \"*info*\") \
				(when (not (string= \"(symbol-value (quote Info-current-file))\" \"$(INFOTN)\")) \
					(info \"$(INFOTN)\")) \
				(revert-buffer t t t)) \
		      (info \"$(INFOTN)\"))"

  html: $(HTML)
  $(HTML): $(TEXI)
	   @echo Making HTML INFO..
	   @makeinfo --html -o $(DIR) $(TEXI)
	   @echo Done making HTML.
	   $(CMPRPL) $(DIR) $(DIR_OLD)
  open-html: html
	   @open $(HTML)

  # If pdftexi2dvi produces an error, it may still produce a viable PDF;
  # therefore, use --tidy.  If it produces an error, try to link the PDF;
  # if it does not produce an error, the PDF will be added to the top dir
  # and there will be no attempt to link.
  pdf:	$(PDF)
  $(PDF): $(TEXI)
	  @echo Making PDF INFO...
	  @-pdftexi2dvi --quiet --build=tidy $(TEXI) || ln -s $(PROJ).t2d/pdf/build/$(PDF) $(PDF)
	  @echo Done making PDF.
  open-pdf:pdf
	   @open $(PDF)

  sync:   $(HTML)
	  @echo Syncing version $(VERS) onto $(S3VERS)...
	  $(S3SYNC)
	  $(S3IMAGESYNC)
	  @echo Done syncing.
	  [[ $(VERS) != $(S3VERS) ]] && { echo Moving...; $(S3MOVE); echo Done moving.;  make homepage; } || :
	  [[ $(PROJINS3) = "NO" ]] && make homepage || :

  # This is a target-specific variable for updating the ‚Äúdescription‚Äù
  # key on the GitHub repo page with the current version number.  It
  # first makes a curl call to the GitHub project repo, finds the
  # ‚Äúdescription‚Äù line, pulls out the description only (leaving the old
  # version) and then prints the value with the current version number.
  # This value is used by the ‚Äúhomepage:‚Äù target in the PATCH call.
  # This method is arguably harder to code but faster to run than using
  # Perl with the JSON::PP module.

  homepage: description = $(shell \
	  curl -s \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S | \
		  (perl -ne 'if (/^\s*\"description\":\s*\"(.*): v(?:(?:[[:digit:]]+[.]?){3})/) {print $$1}'))

  ### NOTE the use of the S variable at the end of PROJ; this is to handle
  # the singular case of the GitHub repo using the plural form, Templates
  # whereas the the Template.org file uses the singular form.
  homepage: $(ORG) upload-index-html
	    @echo Updating homepage...
	    @echo DESCRIPTION: $(description)
	    @echo VERS: $(VERS)
	    @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Content-Type: application/json" \
		  -X PATCH \
		  -d "{\"homepage\":\"$(HTTPS_BUCKET)/$(DIR)-$(VERS)\",\
		       \"description\":\"$(description): $(VERS)\"}" \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	    @echo Done updating homepage.

  delete-proj:
	  @echo Deleting project $(PROJ)...
	  @curl -i \
		  -H "Authorization: token $(GITHUB_TOKEN)" \
		  -H "Accept: application/vnd.github.v3+json" \
		  -X DELETE \
		  https://api.github.com/repos/$(USER)/$(PROJ)$S
	  @$(S3REMOVE)
	  @make dist-clean
	  @make upload-index-html
	  @$(EDITOR) -u --eval "(kill-buffer \"$(ORG)\")"
	  @rm -rf "../$(PROJ)"
	  @echo Done deleting project.

  index-html: $(INDEX)
  $(INDEX): $(ORG)
	  @echo making index.html...
	  $(EDITOR) --eval \
	  "(with-current-buffer (find-file-noselect \"$(ORG)\") \
		  (save-excursion \
		    (org-link-search \"#project-index-title\") \
		    (org-export-to-file (quote html) \"index.html\" nil t)))"
	  @echo Done making index.html.

  upload-index-html: $(INDEX)
	   @echo Uploading index.html...
	   $(S3COPY)
	   @echo Done uploading index.html

  install-aws-cli:
	    curl "https://awscli.amazonaws.com/AWSCLIV2.pkg" -o "AWSCLIV2.pkg" && \
	    sudo installer -pkg AWSCLIV2.pkg -target / && \
	    which aws && aws --version
	    rm -rf AWSCLIV2.pkg

  clean:
	  @echo Cleaning...
	    -@rm *~ 2>/dev/null
	    -@for file in *.??*; \
	    do \
		    ext=$${file#$(PROJ).}; \
		    [[ ! $${ext} =~ org|texi|info|pdf|html ]] && rm -rv $${file}; \
	    done

  dist-clean: clean
	  @echo Dist Cleaning...
	    @${EDITOR} -u --eval \
	      "(kill-buffer \"$(ORG)\")"
	    -@rm -rf *.{texi*,info*,html*,pdf*} $(DIR) $(TOOLS)
	    -@for dir in *; \
		do \
		    [ -d $$dir -a $$dir != "$(DIR_OLD)" -a $$dir != $(SAVE) ] && \
		    rm -vr $$dir; \
		done

  wipe-clean: dist-clean
	  @echo Wipe Clean...
	    -@rm -rf Makefile Readme.md $(DIR_OLD)
	    @git checkout Makefile README.md

  git-ready: dist-clean
	    git checkout Makefile
	    git checkout README.md
	    git status

  help:
	    @echo '"make boot" tangles all of the files in Template'
	    @echo '"make default" makes the .texi file, the .info file, \
	    the html files, and the .pdf file.'
	    @echo

	    @echo '"make check" checks for prerequistes'
	    @echo '"make values" runs check and prints variable values'
	    @echo

	    @echo '"make texi" makes the .texi file'
	    @echo '"make info" makes the .info file'
	    @echo '"make html" makes the html distribution in a subdirectory'
	    @echo '"make pdf" makes the .pdf file'
	    @echo

	    @echo '"make open-org" opens the ORG program using emacsclient for editing'
	    @echo '"make open-texi" opens the .texi file using emacsclient for review'
	    @echo '"make open-html" opens the distribution index.html file \
	    in the default web browser'
	    @echo '"make open-pdf" opens the .pdf file'
	    @echo

	    @echo '"make sync" syncs the html files in the AWS S3 bucket BUCKET; \
	    you must have your AWS S3 bucket name in the env var AWS_S3_BUCKET; \
	    You must have your AWS credentials installed in ~/.aws/credentials'
	    @echo

	    @echo '"make install-aws-cli" installs the "aws cli v2" command-line tools'
	    @echo 'You also need to run "aws configure" and supply your Access Key and Secret Access Key'
	    @echo

	    @echo '"make clean" removes the .texi, .info, and backup files ("*~")'
	    @echo '"make dist-clean" cleans, removes the html distribution, \
	    and removes the build directory'
	    @echo '"make wipe-clean" wipes clean the directory, including old directories'
	    @echo

	    @echo '"make delete-proj" deletes the project from the file system, GitHub and AWS'

#+end_src

*** TODO Next
1. The CloudFront configuration needs to be updated recognize the new version
   directory that is created as part of the ~sync~ operation.

2. Update the GitHub HOME website link for each new sync operation.

3. Store on GitHub a version of each other format upon a sync operation (i.e.,
   the INFO and PDF versions)

** Compare Replace

#+begin_comment
The following source code tangles all files during an export operation. This is
to  make  sure  the  ~cmprpl~  source code  exists  in  the  ~resources/tools/~
directory before running  the Makefile target =html=. It also  makes sure there
is a Makefile on an initial export. The following code is not exported.
#+end_comment

#+name:tangle-org-file
#+header: :exports results :eval yes :results silent
#+begin_src emacs-lisp
(org-babel-tangle-file (buffer-file-name))
#+end_src

The  AWS ~sync~  command  relies  upon time  stamps  to  determine whether  two
programs are identical or not, as  well as content.  If two otherwise identical
files have  different time stamps,  ~sync~ will  assume they are  different and
will  process the  newer.   However, the  ~texinfo~  ~makeinfo --html~  command
produces all  new files even  if some files  (or most files)  remain unchanged.
This  means that  all files  will be  uploaded to  the AWS  S3 bucket  on every
iteration, even though the majority of the files are actually unchanged.

The ~cmprpl~  source code attempts to  resolve the issue of  identical exported
code having different  time stamps, thus defeating the benefit  provided by the
~aws2 s3 sync~ command uploading only changed files.

This program makes sure that a generated HTML directory exists: =$DIR_NEW=.  If
it doesn‚Äôt, then it is in an improper state and the program stops with an error
message.

The  program then  checks  if  an old  directory  exists,  =$DIR_OLD=.  If  one
doesn‚Äôt,  then one  is  created by  copying the  current  new directory.   This
provides a baseline  for comparisons going forward.  The program  exits at that
point. It is very important that  the =$DIR_OLD= directory not be deleted going
forward.

Given  that =$DIR_OLD=  exists, the  program then  loops through  all files  in
=$DIR_NEW= and  compares them  to the  files in =$DIR_OLD=.   If the  files are
identical, the =$DIR_OLD= file replaces the =$DIR_NEW= file while retaining the
old time stamp (using the ~-p~ option of ~cp~. If a file is different, then the
=$DIR_NEW= file  replaces the =$DIR_OLD=  file, thus giving it  updated content
and  an updated  time stamp.   If the  file does  not exist  in the  =$DIR_OLD=
directory, then it is added.

The  program then  loops through  all of  the files  in the  old directory  and
deletes  any that  do not  exist in  the new  directory.  Now  both directories
should be in sync.

#+caption:Compare Replace program
#+name:cmprpl
#+header: :mkdirp t
#+header: :shebang "#!/usr/bin/env bash"
#+begin_src sh :tangle resources/tools/cmprpl
  [[ $# -eq 2 ]] || { echo "ERROR: Incorrect command line arguments"; exit 1; }
  DIR_NEW=$1
  DIR_OLD=$2

  [[ -d $DIR_NEW ]] || { echo "ERROR: $DIR_NEW does not exist"; exit 1; }
  [[ -d $DIR_OLD ]] || { echo "CREATING: $DIR_OLD does not exist"; cp -a $DIR_NEW $DIR_OLD; exit 0; }

  for newfile in $DIR_NEW/*
  do
      oldfile=$DIR_OLD/$(basename $newfile)
      if [[ -e $oldfile ]]
      then
	 if cmp -s $newfile $oldfile
	 then
	     printf "${GREEN}copying OLD to NEW${CLEAR}: "
	     cp -vp $oldfile $newfile
	 else
	     printf "${PURPLE}copying NEW to OLD${CLEAR}: "
	     cp -vp $newfile $oldfile
	 fi
      else
	  printf "${BLUE}creating NEW in OLD${CLEAR}: "
	  cp -vp $newfile $oldfile
      fi
  done

  for oldfile in $DIR_OLD/*
  do
      newfile=$DIR_NEW/$(basename $oldfile)
      if [[ ! -e $newfile ]]
      then
	  printf "${RED}removing OLD${CLEAR}: "
	  rm -v $oldfile
      fi
  done
#+end_src


** Update Utility Commands
*** Get Parsed Org Tree
This function looks for an Org file in the present working directory, and if it
finds one returns  a parsed tree using  ~org-element-parse-buffer~.  It returns
=nil= if there is no Org file or if the found file is not in ~org-mode~.

#+name:get-parsed-org-tree
#+header: :results silent
#+begin_src emacs-lisp
(defun get-parsed-org-tree (&optional org-dir)
  "This function takes an optional directory name, changes to
that directory if given, otherwise uses the pwd, and finds an Org
file and returns its parsed tree, or nil if none found."
  (when org-dir
      (cd (file-name-as-directory org-dir)))
  (let ((buf (car-safe (find-file-noselect "*.org" nil nil t))))
    (if buf
	(with-current-buffer buf (org-element-parse-buffer))
      nil)))
#+end_src

*** Check for CID
This code  checks whether an  Org file contains  a =custom_id= of  a particular
value.  It accepts  a ~cid-value~ and an optional directory.   If the directory
is not given, then it defaults to the current directory.  If throws an error if
the directory does not exist.  It returns =nil= if the given directory does not
contain an Org file.   It returns =t= if the Org file  contains a node property
of   =custom_id=  and   value  ~cid-value~,   or   =nil=  if   not.   It   uses
~get-parsed-org-tree~.

#+name:org-tree-cid-p
#+header: :results silent
#+begin_src emacs-lisp
(defun org-tree-cid-p (cid-value &optional org-dir)
  "Check whether an org file contains a custom_id of CID"
  (let ((tree (get-parsed-org-tree org-dir)))
    (car (org-element-map tree 'property-drawer
	   (lambda (pd) (org-element-map (org-element-contents pd) 'node-property
			  (lambda (np)
			    (and
			     (string= "custom_id" (org-element-property :key np))
			     (string= cid-value (org-element-property :value np))))))
	   nil t))))
#+end_src

#+name:run-org-tree-cid-p
#+header: :var cid="build-tools"
#+header: :var dir="/usr/local/dev/programming/MasteringEmacs"
#+header: :var gpot=get-parsed-org-tree()
#+header: :var otcp=org-tree-cid-p()
#+header: :results value
#+header: :eval never-export
#+begin_src emacs-lisp
(org-tree-cid-p cid dir)
#+end_src

#+call: run-org-tree-cid-p(dir="/usr/local/dev/programming/MasteringEmacs")

** Bucket Index HTML
The bucket should contain a master ~index.html~  file that links to each of the
individual project  ~index.html~ files.  The  master ~index.html~ file  will be
placed at the root of  the bucket, ~https://<bucket-name>.com/~, and the bucket
must be set up to serve this ~index.html~ when the user hits the root.

*** Get Bucket Name
 This  code searches  for  the keyword-value  pair =bucket:<BUCKET-NAME>=  that
 should be  located towards the  beginning of the  file, and returns  the value
 =BUCKET-NAME= or nil if not found.

#+name: get-bucket-name
#+header: :results value
#+begin_src emacs-lisp
   (save-excursion
     (goto-char (point-min))
     (re-search-forward "^#\\+bucket:\\s*?\\(.*\\)$" nil t)
     (match-string-no-properties 1))
#+end_src

For some reason, ~get-bucket-name~ does not  work when called from the headline
[[#project-index-links][=Links for  bucket=]] below  when creating  =index.html=, even  if it  returns as
~(prin1 ...)~ and is  set up to ~:return output~; the  call receives =nil=. The
following code from ~bucket-name~, however, works. I don't know why.

#+name: bucket-name
#+header: :results output
#+header: :var bucket-name=get-bucket-name()
#+begin_src emacs-lisp
(prin1 bucket-name)
#+end_src

*** Bucket HTTPS URL
This  code calls  ~get-bucket-name~ and  returns the  value returned  as a  URL
string or nil.

#+name: bucket-https-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "https://" b)
#+end_src

*** S3 Bucket URL
This code calls ~get-bucket-name~ and returns the AWS S3 bucket url.

#+name: s3-bucket-url
#+header: :results value
#+header: :var b=get-bucket-name()
#+begin_src emacs-lisp
(concat "s3://" b)
#+end_src

*** Bucket Projects List
This code uses the ~s3-bucket-url~ result to obtain the list of projects in the
bucket.  It does  this by calling the  AWS S3 high-level command  ~ls~ and then
removing the  =PRE= string in  each result.  The result  that is returned  is a
single  string that  can be  separated into  individual links  by breaking  the
string on spaces.

#+name: bucket-projects-list
#+header: :results output
#+header: :var bucket=s3-bucket-url()
#+begin_src sh
/usr/local/bin/aws s3 ls ${bucket} | sed -ne 's/^.*PRE //p'
#+end_src

*** Bucket Project Links
This code  uses the result  from ~bucket-projects-list~ to create  an unordered
list of  links written to  bucket projects, written  in Org-mode syntax.  It is
executed by a =#+call:= in [[*Bucket Index][*Bucket  Index]] during an HTML export of that subtree
to a file called =index.html=.

#+name: bucket-project-links
#+header: :var b-url=bucket-https-url()
#+header: :var projects=bucket-projects-list()
#+header: :results output raw
#+begin_src emacs-lisp
(seq-do (lambda (u) (princ (format "- [[%s/%sindex.html][~%s~]]
" b-url u u))) (split-string projects))
#+end_src

*** Bucket Index
    :PROPERTIES:
    :custom_id: project-index-title
    :export_file_name: index.html
    :export_subtitle: {{{version}}} created {{{upload-date}}}
    :END:
#+html_doctype: html5
#+options: toc:nil html5-fancy:t

#+html: <hr>

**** Links for bucket call_bucket-name()
     :PROPERTIES:
     :unnumbered: t
     :custom_id: project-index-links
     :END:

#+call: bucket-project-links()
** Project Readme
This adds the README.md template to a project. It should be customized uniquely
for the project.

#+name:project-readme
#+header: :tangle README.md
#+begin_src markdown
# TITLE
## Subtitle
## Author
## Date
## Version
# ABSTRACT
This is the Org Template file.	It is the parent of all other Org Info blogs,
and provides the source code for processing them in various different ways.
# INTRODUCTION
# CHAPTER
## Section
### Subsection
#+end_src

** Boot Template
:PROPERTIES:
:dependency1: EMACS:=:/Applications/MacPorts/Emacs.app/Contents/MacOS/Emacs or similar
:dependency2: EDITOR:=:emacsclient
:dependency3: =SYNC_ORG_TEMPLATE= defined as $DEV/Templates/Org/Template.org
:END:
Although running the command ~org-babel-tangle~ (=C-c C-v t=) from within Emacs
will install  everything, it would  be nice to have  a simple Makefile  that is
downloaded with this  file that could be  invoked to do the  same thing without
starting Emacs and Org-mode and keying in the ~org-babel-tangle~ command.  This
little Makefile should be stored on  GitHub along with the ~Template.org~ file.
When  the source  is extracted  to a  directory, then  running this  Makefile's
default rule  as simply ~make~  will extract the ~preprocess.el~  script, which
updates  =DEV= and  then  extracts the  full Makefile.   Because  this file  is
tangled along with the full Makefile, it simply gets tacked onto the end of the
big Makefile as an additional rule.   Now, running ~make~ runs the default rule
from the  main Makefile, which is  to extract everything, then  export to TEXI,
INFO, HTML, and PDF forms.

It is assumed that an Emacs server is running, and that the $EDITOR environment
variable is set to use ~emacsclient~.

#+name:boot-template
#+header: :tangle Makefile
#+begin_src makefile
  boot:
	  $(EDITOR) -u --eval \
		  "(with-current-buffer (car (find-file-noselect \"./*.org\" nil nil t)) \
			  (goto-char (point-min)) \
			  (re-search-forward \"^#[+]name:preprocess.el$$\") \
			  (org-babel-tangle (quote (4))) \
			  (save-buffer) \
			  (kill-buffer))" \
	  --eval \
		  "(let ((rsrcdir \"resources\") \
			 (subdirs (list \"tools\" \"images\"))) \
		     (mkdir rsrcdir t) \
		     (dolist (subdir subdirs) (mkdir (concat rsrcdir \"/\" subdir) t)))"
	  ./resources/tools/preprocess.el
#+end_src

** Preprocess Env Vars
The environment variable DEV can be  in different locations and will be spelled
differently based  on how the  local machine is set  up.  For instance,  on one
system,  it will  be at  ~$HOME/Dev~  while in  another  system it  will be  at
~/usr/local/dev~.  However, the =:tangle= keyword  does not expand variables in
the form ~${DEV}~,  but rather requires absolute  paths, like ~/usr/local/dev~.
Therefore, this program works like a preprocessor for environment variables set
up  as part  of  =:tangle= lines,  changing them  to  their system  environment
variable values prior to tangling.  It lives in the ~resources/tools~ directory.

#+name:preprocess.el
#+header: :mkdirp t
#+header: :tangle resources/tools/preprocess.el
#+header: :shebang "#!/opt/local/bin/emacs -Q --script"
#+begin_src emacs-lisp
  (with-current-buffer (car (find-file-noselect "./*.org" nil nil t))
    (save-excursion
    (goto-char (point-min))
    (let ((re-search-str "\\(?::tangle\\|load-file \\(?:[\\]*\\)?[\"]\\)\s*\\(.*?/[dD]ev\\)/")
          (dev (getenv "DEV")))
      (while
              (re-search-forward re-search-str nil t)
              (replace-match dev t nil nil 1)))
    (save-buffer)
    (require 'org)
    (org-babel-tangle)))
#+end_src

** Samples
#+begin_comment
(cd "~/Dev/Emacs/MasteringEmacs/")
"/Users/pine/Dev/Emacs/MasteringEmacs/"

(defun add-bucket (org bucket)
  "Add a bucket keyword BUCKET to the org file ORG."
  (interactive "fFile: \nsBUCKET: ")
  (with-current-buffer (find-file-noselect org)
    (let* ((tree (org-element-parse-buffer))
	   (ins (car (org-element-map tree (quote section)
		 (lambda (s)
		   (org-element-map s (quote keyword)
		     (lambda (kw) (when (equal "MACRO" (org-element-property :key kw)) (1- (org-element-property :end kw))))
		     nil nil :keyword))
		 nil t nil nil))))
      (goto-char ins)
      (insert (format "#+bucket:%s\n" bucket))
      ())))

(add-bucket "MasteringEmacs.org" "pinecone-forest")
nil

(defun hl-region (raw-hl)
  "Obtain the begin and end positions for a headline."
  (with-current-buffer (find-file-noselect (getenv "SYNC_ORG_TEMPLATE"))
    (let* ((tree (get-parsed-tree))
	   (hl (car-safe (org-element-map tree 'headline
			   (lambda (hl) (when
					    (string= raw-hl
						     (org-element-property :raw-value hl))
					  (org-element-context)))
			   nil nil t))))
      (cons
       (org-element-property :begin hl)
       (org-element-property :end hl))
      )))

(hl-region "Build Tools")

(4888 . 29646)

(defun get-hl-with-prop (org-dir hl-prop)
  "Given a directory containing an Org template file and a custom_id property name, return the headline containing that custom_id, or nil if none."
  (progn
    (cd org-dir)
    (let ((org-buf (car-safe (find-file-noselect "*.org" nil nil t))))
      (if org-buf
	  (with-current-buffer org-buf
	    (let ((tree (org-element-parse-buffer)))
	      (org-element-map tree 'headline
		(lambda (hl)
		  (let ((cid (org-element-property :CUSTOM_ID hl)))
		    (when (string= hl-prop cid)
		      (and
		       (message (format "Found the headline %s containing property %s." (org-element-property :raw-value hl) hl-prop))
		       hl))))
		nil t)))
	(and
	 (message (format "The directory %s does not contain an Org file." org-dir))
	 nil)))))

(get-hl-with-prop "~/Dev/Templates/Org" "build-tools")

(headline (:raw-value "Build Tools" :begin 4888 :end 29646 :pre-blank 0 :contents-begin 4902 :contents-end 29645 :level 1 :priority nil :tags nil :todo-keyword nil :todo-type nil :post-blank 1 :footnote-section-p nil :archivedp nil :commentedp nil :post-affiliated 4888 :FROM-FILE "Template" :CUSTOM_ID "build-tools" :APPENDIX "t" :title "Build Tools"))









;;; Add a keyword named 'bucket' just after the version macro.
;;; This function should be run from within the directory containing the Org file.
(defun add-bucket (org-file s3-bucket)
  "Add the name of the associated AWS S3 bucket to an Org templated file."
  (with-current-buffer (find-file-noselect org-file)
    (goto-char (point-min))
    (let* ((tree (org-element-parse-buffer))
	   ;; find the beginning position of the first headline to act as a limit
	   (hl1 (org-element-map tree (quote headline) (lambda (hl) (org-element-property :begin hl)) nil t)))
      ;; Check for the presence of a bucket keyword before the first headline
      (unless (re-search-forward "^#\\+bucket:" hl1 t)
	;; If no bucket keyword is found, search for a keyword MACRO with the value 'version'
	(org-element-map tree (quote keyword)
	  (lambda (kw) (when (and (string= "MACRO" (org-element-property :key kw))
				  (string-match-p "version" (org-element-property :value kw)))
			 ;; return the end position of the MACRO; subtract an empty line if there is one
			 (goto-char (- (org-element-property :end kw) (org-element-property :post-blank kw)))
			 (insert "#+bucket:" s3-bucket)
			 (newline)
			 (basic-save-buffer)
			 (message (format "Added bucket %s" s3-bucket))))
	  nil t)))))

(add-bucket "MasteringEmacs.org" "pinecone-forest.com")
nil

"Added bucket pinecone-forest.com"









(keyword (:key "MACRO" :value "version Version 0.0.108" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...))
("TITLE" "SUBTITLE" "AUTHOR" "DATE" "MACRO" "TEXINFO" "TEXINFO" "CINDEX" "CINDEX" "CINDEX" "CINDEX" "CINDEX" ...)







((keyword (:key "MACRO" :value "version Version 0.0.107" :begin 148 :end 181 :post-blank 1 :post-affiliated 148 ...)))
#+end_comment

* List of Programs
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Listing

* List of Examples
:PROPERTIES:
:appendix: t
:END:
#+texinfo:@listoffloats Example

* Copying
:PROPERTIES:
:copying:  t
:END:

Copyright \copy 2020 by {{{author}}}

* Concept Index
:PROPERTIES:
:index: cp
:appendix: yes
:END:

* Program Index
:PROPERTIES:
:index: pg
:appendix: yes
:END:

* Function Index
:PROPERTIES:
:index: fn
:appendix: yes
:END:

* Variable Index
:PROPERTIES:
:index: vr
:appendix: yes
:END:


* Configuration							   :noexport:
#+startup:content

#+todo: SOMEDAY(s@) TODO(t@) INPROGRESS(i@) WAIT(w@) | CANCEL(c@) DONE(d!)

#+options: H:4

#+texinfo_class: info
#+texinfo_header:
#+texinfo_post_header:
#+texinfo_dir_category:<DIR CATEGORY>
#+texinfo_dir_title:<DIR TITLE>
#+texinfo_dir_desc:<DIR DESCRIPTION>
#+texinfo_printed_title:OAuth2---All About OAuth2


* Footnotes

[fn:1]In the browser, add =index.text= to the end of the URL to see the source.

[fn:2]Markdown requires the standard Perl library module Digest::MD5.


* Local Variables						   :noexport:
# Local Variables:
# fill-column: 79
# indent-tabs-mode: t
# eval: (auto-fill-mode)
# time-stamp-pattern: "8/^\\#\\+date:%:y-%02m-%02d %02H:%02M$"
# End:
